structure = {
    "01_绪论": [
        {
            "1.1_强化学习概述": [
                "1.1.1_强化学习与监督学习",
                "1.1.2_强化学习的例子",
                "1.1.3_强化学习的历史",
                "1.1.4_强化学习的应用"
            ]
        },
        {
            "1.2_序列决策": [
                "1.2.1_智能体和环境",
                "1.2.2_奖励",
                "1.2.3_序列决策"
            ]
        },
        {
            "1.3_动作空间": []
        },
        {
            "1.4_强化学习智能体的组成成分和类型": [
                "1.4.1_策略",
                "1.4.2_价值函数",
                "1.4.3_模型"
            ]
        },
        {
            "1.5_学习与规划": []
        },
        {
            "1.6_探索和利用": []
        },
        {
            "1.7_强化学习实验": [
                "1.7.1_Gym",
                "1.7.2_MountainCar-v0_例子"
            ]
        },
        {
            "1.8_关键词": []
        },
        {
            "1.9_习题": []
        },
        {
            "1.10_面试题": []
        }
    ],
    "02_马尔可夫决策过程": [
        {
            "2.1_马尔可夫过程": [
                "2.1.1_马尔可夫性质",
                "2.1.2_马尔可夫链",
                "2.1.3_马尔可夫过程的例子"
            ]
        },
        {
            "2.2_马尔可夫奖励过程": [
                "2.2.1_回报与价值函数",
                "2.2.2_贝尔曼方程",
                "2.2.3_计算马尔可夫奖励过程价值的迭代算法",
                "2.2.4_马尔可夫奖励过程的例子"
            ]
        },
        {
            "2.3_马尔可夫决策过程": [
                "2.3.1_马尔可夫决策过程中的策略",
                "2.3.2_马尔可夫决策过程和马尔可夫过程/马尔可夫奖励过程的区别",
                "2.3.3_马尔可夫决策过程中的价值函数",
                "2.3.4_贝尔曼期望方程",
                "2.3.5_备份图",
                "2.3.6_策略评估",
                "2.3.7_预测与控制",
                "2.3.8_动态规划",
                "2.3.9_马尔可夫决策过程中的策略评估",
                "2.3.10_马尔可夫决策过程控制",
                "2.3.11_策略迭代",
                "2.3.12_价值迭代",
                "2.3.13_策略迭代与价值迭代的区别",
                "2.3.14_马尔可夫决策过程中的预测和控制总结"
            ]
        },
        {
            "2.4_关键词": []
        },
        {
            "2.5_习题": []
        },
        {
            "2.6_面试题": []
        }
    ],
    "03_表格型方法": [
        {
            "3.1_马尔可夫决策过程": [
                "3.1.1_有模型",
                "3.1.2_免模型",
                "3.1.3_有模型与免模型的区别"
            ]
        },
        {
            "3.2_Q_表格": []
        },
        {
            "3.3_免模型预测": [
                "3.3.1_蒙特卡洛策略评估",
                "3.3.2_时序差分",
                "3.3.3_动态规划方法、蒙特卡洛方法以及时序差分方法的自举和采样"
            ]
        },
        {
            "3.4_免模型控制": [
                "3.4.1_Sarsa：同策略时序差分控制",
                "3.4.2_Q_学习：异策略时序差分控制",
                "3.4.3_同策略与异策略的区别"
            ]
        },
        {
            "3.5_使用_Q_学习解决悬崖寻路问题": [
                "3.5.1_CliffWalking-v0_环境简介",
                "3.5.2_强化学习基本接口",
                "3.5.3_Q_学习算法",
                "3.5.4_结果分析"
            ]
        },
        {
            "3.6_关键词": []
        },
        {
            "3.7_习题": []
        },
        {
            "3.8_面试题": []
        }
    ],
    "04_策略梯度": [
        {
            "4.1_策略梯度算法": []
        },
        {
            "4.2_策略梯度实现技巧": [
                "4.2.1_技巧_1：添加基线",
                "4.2.2_技巧_2：分配合适的分数"
            ]
        },
        {
            "4.3_REINFORCE：蒙特卡洛策略梯度": []
        },
        {
            "4.4_关键词": []
        },
        {
            "4.5_习题": []
        },
        {
            "4.6_面试题": []
        }
    ],
    "05_近端策略优化": [
        {
            "5.1_重要性采样": []
        },
        {
            "5.2_近端策略优化": [
                "5.2.1_近端策略优化惩罚",
                "5.2.2_近端策略优化裁剪"
            ]
        },
        {
            "5.3_关键词": []
        },
        {
            "5.4_习题": []
        },
        {
            "5.5_面试题": []
        }
    ],
    "06_深度_Q_网络": [
        {
            "6.1_状态价值函数": []
        },
        {
            "6.2_动作价值函数": []
        },
        {
            "6.3_目标网络": []
        },
        {
            "6.4_探索": []
        },
        {
            "6.5_经验回放": []
        },
        {
            "6.6_深度_Q_网络": []
        },
        {
            "6.7_关键词": []
        },
        {
            "6.8_习题": []
        },
        {
            "6.9_面试题": []
        }
    ],
    "07_深度_Q_网络进阶技巧": [
        {
            "7.1_双深度_Q_网络": []
        },
        {
            "7.2_竞争深度_Q_网络": []
        },
        {
            "7.3_优先级经验回放": []
        },
        {
            "7.4_在蒙特卡洛方法和时序差分方法中取得平衡": []
        },
        {
            "7.5_噪声网络": []
        },
        {
            "7.6_分布式_Q_函数": []
        },
        {
            "7.7_彩虹": []
        },
        {
            "7.8_使用深度_Q_网络解决推车杆问题": [
                "7.8.1_CartPole-v0_简介",
                "7.8.2_深度_Q_网络基本接口",
                "7.8.3_回放缓冲区",
                "7.8.4_Q_网络",
                "7.8.5_深度_Q_网络算法",
                "7.8.6_结果分析"
            ]
        },
        {
            "7.9_关键词": []
        },
        {
            "7.10_习题": []
        },
        {
            "7.11_面试题": []
        }
    ],
    "08_针对连续动作的深度_Q_网络": [
        {
            "8.1_方案_1：对动作进行采样": []
        },
        {
            "8.2_方案_2：梯度上升": []
        },
        {
            "8.3_方案_3：设计网络架构": []
        },
        {
            "8.4_方案_4：不使用深度_Q_网络": []
        },
        {
            "8.5_习题": []
        }
    ],
    "09_演员-评论员算法": [
        {
            "9.1_策略梯度回顾": []
        },
        {
            "9.2_深度_Q_网络回顾": []
        },
        {
            "9.3_优势演员-评论员算法": []
        },
        {
            "9.4_异步优势演员-评论员算法": []
        },
        {
            "9.5_路径衍生策略梯度": []
        },
        {
            "9.6_与生成对抗网络的联系": []
        },
        {
            "9.7_关键词": []
        },
        {
            "9.8_习题": []
        },
        {
            "9.9_面试题": []
        }
    ],
    "10_稀疏奖励": [
        {
            "10.1_设计奖励": []
        },
        {
            "10.2_好奇心": []
        },
        {
            "10.3_课程学习": []
        },
        {
            "10.4_分层强化学习": []
        },
        {
            "10.5_关键词": []
        },
        {
            "10.6_习题": []
        }
    ],
    "11_模仿学习": [
        {
            "11.1_行为克隆": []
        },
        {
            "11.2_逆强化学习": []
        },
        {
            "11.3_第三人称视角模仿学习": []
        },
        {
            "11.4_序列生成和聊天机器人": []
        },
        {
            "11.5_关键词": []
        },
        {
            "11.6_习题": []
        }
    ],
    "12_深度确定性策略梯度": [
        {
            "12.1_离散动作与连续动作的区别": []
        },
        {
            "12.2_深度确定性策略梯度": []
        },
        {
            "12.3_双延迟深度确定性策略梯度": []
        },
        {
            "12.4_使用深度确定性策略梯度解决倒立摆问题": [
                "12.4.1_Pendulum-v1_简介",
                "12.4.2_深度确定性策略梯度基本接口",
                "12.4.3_Ornstein-Uhlenbeck_噪声",
                "12.4.4_深度确定性策略梯度算法",
                "12.4.5_结果分析"
            ]
        },
        {
            "12.5_关键词": []
        },
        {
            "12.6_习题": []
        },
        {
            "12.7_面试题": []
        }
    ],
    "13_AlphaStar_论文解读": [
        {
            "13.1_AlphaStar_以及背景简介": []
        },
        {
            "13.2_AlphaStar_的模型输入和输出是什么呢？————环境设计": [
                "13.2.1_状态（网络的输入）",
                "13.2.2_动作（网络的输出）"
            ]
        },
        {
            "13.3_AlphaStar_的计算模型是什么呢？————网络结构": [
                "13.3.1_输入部分",
                "13.3.2_中间过程",
                "13.3.3_输出部分"
            ]
        },
        {
            "13.4_庞大的_AlphaStar_如何训练呢？————学习算法": [
                "13.4.1_监督学习",
                "13.4.2_强化学习",
                "13.4.3_模仿学习",
                "13.4.4_多智能体学习/自学习"
            ]
        },
        {
            "13.5_AlphaStar_实验结果如何呢？————实验结果": [
                "13.5.1_宏观结果",
                "13.5.2_其他实验（消融实验）"
            ]
        },
        {
            "13.6_关于_AlphaStar_的总结": []
        }
    ],
    "附录": [
        {
            "A_习题解答": []
        },
        {
            "B_面试题解答": []
        }
    ]
}


import os
from typing import Dict, Any

def create_directories_and_files(
        base_path: str, 
        structure: Dict[str, Any], 
        readme_file, 
        parent_path: str = "", 
        level: int = 1
    ):
    heading = "#" * level

    for key, value in structure.items():
        current_path = os.path.join(base_path, key.replace(" ", "_").replace("/", "_").replace("-", "_"))

        # 创建目录
        os.makedirs(current_path, exist_ok=True)

        # 在README中添加章节标题
        if parent_path:
            readme_file.write(f"{heading} {parent_path}/{key}\n\n")
        else:
            readme_file.write(f"{heading} {key}\n\n")

        # 递归调用创建子目录和文件
        if isinstance(value, dict) and value:
            create_directories_and_files(
                current_path, 
                value, 
                readme_file, 
                parent_path + "/" + key if parent_path else key, 
                level + 1
            )
        elif isinstance(value, list) and value:
            for idx, item in enumerate(value):
                if isinstance(item, dict) and item:
                    create_directories_and_files(
                        current_path, 
                        item, 
                        readme_file, 
                        parent_path + "/" + key if parent_path else key, 
                        level + 1
                    )
                else:
                    item = f"{idx:02d}_{item}"
                    file_name = item.replace(" ", "_").replace("/", "_").replace("-", "_") + ".py"
                    file_path = os.path.join(current_path, file_name)
                    with open(file_path, 'w', encoding='utf-8') as file:
                        file.write(f"# {item}\n\n")
                        file.write(f'"""\nLecture: {parent_path}/{key}\nContent: {item}\n"""\n\n')

                    # 在README中添加文件链接
                    item_clean = item.replace(" ", "_").replace("/", "_").replace("-", "_")
                    parent_clean = parent_path.replace(" ", "_").replace("/", "_").replace("-", "_")
                    key_clean = key.replace(" ", "_").replace("/", "_").replace("-", "_")
                    readme_file.write(f"- [{item}](./{parent_clean}/{key_clean}/{item_clean}.py)\n")
                    
                    
                    file_name = item.replace(" ", "_").replace("/", "_").replace("-", "_") + ".md"
                    file_path = os.path.join(current_path, file_name)
                    with open(file_path, 'w', encoding='utf-8') as file:
                        file.write(f"# {item}\n\n")
                        file.write(f'"""\nLecture: {parent_path}/{key}\nContent: {item}\n"""\n\n')

                    # 在README中添加文件链接
                    item_clean = item.replace(" ", "_").replace("/", "_").replace("-", "_")
                    parent_clean = parent_path.replace(" ", "_").replace("/", "_").replace("-", "_")
                    key_clean = key.replace(" ", "_").replace("/", "_").replace("-", "_")
                    readme_file.write(f"- [{item}](./{parent_clean}/{key_clean}/{item_clean}.md)\n")
        else:
            # 创建文件并写入初始内容
            file_name = key.replace(" ", "_").replace("/", "_").replace("-", "_") + ".py"
            file_path = os.path.join(current_path, file_name)
            with open(file_path, 'w', encoding='utf-8') as file:
                file.write(f"# {key}\n\n")
                file.write(f'"""\nLecture: {parent_path}/{key}\nContent: {key}\n"""\n\n')

            # 在README中添加文件链接
            parent_clean = parent_path.replace(" ", "_").replace("/", "_").replace("-", "_")
            key_clean = key.replace(" ", "_").replace("/", "_").replace("-", "_")
            readme_file.write(f"- [{key}](./{parent_clean}/{key_clean}/{file_name})\n")
            
            
            file_name = key.replace(" ", "_").replace("/", "_").replace("-", "_") + ".md"
            file_path = os.path.join(current_path, file_name)
            with open(file_path, 'w', encoding='utf-8') as file:
                file.write(f"# {key}\n\n")
                file.write(f'"""\nLecture: {parent_path}/{key}\nContent: {key}\n"""\n\n')

            # 在README中添加文件链接
            parent_clean = parent_path.replace(" ", "_").replace("/", "_").replace("-", "_")
            key_clean = key.replace(" ", "_").replace("/", "_").replace("-", "_")
            readme_file.write(f"- [{key}](./{parent_clean}/{key_clean}/{file_name})\n")

        # 添加空行以分隔不同的章节
        readme_file.write("\n")

def main():
    root_dir = './'
    # 创建根目录
    os.makedirs(root_dir, exist_ok=True)

    # 创建 README.md 文件
    with open(os.path.join(root_dir, "README.md"), 'w', encoding='utf-8') as readme_file:
        readme_file.write("# EasyRL\n\n")
        readme_file.write("这是一个关于EasyRL的目录结构。\n\n")
        create_directories_and_files(root_dir, structure, readme_file)

    print("目录和文件结构已生成，并创建 README.md 文件。")

if __name__ == "__main__":
    main()